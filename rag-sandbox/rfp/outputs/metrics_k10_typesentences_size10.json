{
  "full_dataset": {
    "full_dataset": {
      "answer_relevancy": 0.7305370248224624,
      "faithfulness": 0.8199494949494951,
      "context_recall": 0.9212121212121211,
      "context_precision": 0.694783509666119,
      "answer_correctness": 0.7179126123232354,
      "EM": 0.09090909090909091,
      "F1": 0.5042495726770052,
      "avg_retrieve_context": 0.06769820776852696,
      "avg_llm_response": 1.2825697313655506,
      "avg_total": 1.3502679391340775,
      "sample_size": 110
    }
  },
  "RFP_id": {
    "infra_1": {
      "answer_relevancy": 0.676910441893845,
      "faithfulness": 0.8452380952380952,
      "context_recall": 0.9682539682539681,
      "context_precision": 0.7546823296862706,
      "answer_correctness": 0.6417261716724989,
      "EM": 0.0,
      "F1": 0.336953938813891,
      "avg_retrieve_context": 0.06769820776852693,
      "avg_llm_response": 1.7149698053087508,
      "avg_total": 1.7826680130772778,
      "sample_size": 21
    },
    "infra_2": {
      "answer_relevancy": 0.7237679411429491,
      "faithfulness": 0.6666666666666666,
      "context_recall": 0.8461538461538461,
      "context_precision": 0.7695482295145258,
      "answer_correctness": 0.7467562280813667,
      "EM": 0.07692307692307693,
      "F1": 0.5520714013286642,
      "avg_retrieve_context": 0.06769820776852696,
      "avg_llm_response": 1.7212469027592585,
      "avg_total": 1.7889451105277852,
      "sample_size": 13
    },
    "infra_3": {
      "answer_relevancy": 0.8770344551525713,
      "faithfulness": 0.875,
      "context_recall": 0.9166666666666666,
      "context_precision": 0.7771626983662046,
      "answer_correctness": 0.6472462177670754,
      "EM": 0.0,
      "F1": 0.4150499159229169,
      "avg_retrieve_context": 0.06769820776852696,
      "avg_llm_response": 0.9339739878972372,
      "avg_total": 1.0016721956657642,
      "sample_size": 12
    },
    "infra_4": {
      "answer_relevancy": 0.8051197962609262,
      "faithfulness": 0.8194444444444445,
      "context_recall": 1.0,
      "context_precision": 0.822426303811743,
      "answer_correctness": 0.649291975139523,
      "EM": 0.0,
      "F1": 0.53591474182673,
      "avg_retrieve_context": 0.06769820776852696,
      "avg_llm_response": 1.3100638730185372,
      "avg_total": 1.377762080787064,
      "sample_size": 14
    },
    "natsec_1": {
      "answer_relevancy": 0.7805051572981889,
      "faithfulness": 0.875,
      "context_recall": 1.0,
      "context_precision": 0.6146709655839865,
      "answer_correctness": 0.6459064836579103,
      "EM": 0.0,
      "F1": 0.40719583323702313,
      "avg_retrieve_context": 0.06769820776852696,
      "avg_llm_response": 0.7996886571248373,
      "avg_total": 0.8673868648933643,
      "sample_size": 6
    },
    "natsec_2": {
      "answer_relevancy": 0.9275193810569136,
      "faithfulness": 0.8,
      "context_recall": 1.0,
      "context_precision": 0.5838888888392917,
      "answer_correctness": 0.8241418106044964,
      "EM": 0.0,
      "F1": 0.5770511841100076,
      "avg_retrieve_context": 0.06769820776852695,
      "avg_llm_response": 1.3593562126159668,
      "avg_total": 1.4270544203844937,
      "sample_size": 5
    },
    "natsec_3": {
      "answer_relevancy": 0.8265673128003352,
      "faithfulness": 0.9375,
      "context_recall": 1.0,
      "context_precision": 0.9270275297241533,
      "answer_correctness": 0.7734696627110107,
      "EM": 0.0,
      "F1": 0.45470239879210006,
      "avg_retrieve_context": 0.06769820776852695,
      "avg_llm_response": 0.8082789182662964,
      "avg_total": 0.8759771260348233,
      "sample_size": 8
    },
    "natsec_4": {
      "answer_relevancy": 0.8121376458194548,
      "faithfulness": 0.7777777777777777,
      "context_recall": 0.8333333333333334,
      "context_precision": 0.7719576719107204,
      "answer_correctness": 0.7026347269301648,
      "EM": 0.16666666666666666,
      "F1": 0.58494227994228,
      "avg_retrieve_context": 0.06769820776852696,
      "avg_llm_response": 0.6942055622736613,
      "avg_total": 0.7619037700421883,
      "sample_size": 6
    },
    "natsec_5": {
      "answer_relevancy": 0.9623357214345006,
      "faithfulness": 1.0,
      "context_recall": 0.75,
      "context_precision": 0.6177248677109,
      "answer_correctness": 0.7248427369290446,
      "EM": 0.0,
      "F1": 0.5393192688662601,
      "avg_retrieve_context": 0.06769820776852696,
      "avg_llm_response": 1.9165979623794556,
      "avg_total": 1.9842961701479824,
      "sample_size": 6
    },
    "natsec_6": {
      "answer_relevancy": 0.721827566001475,
      "faithfulness": 0.75,
      "context_recall": 0.875,
      "context_precision": 0.8773809523411547,
      "answer_correctness": 0.6754683360484004,
      "EM": 0.0,
      "F1": 0.32991949910554563,
      "avg_retrieve_context": 0.06769820776852695,
      "avg_llm_response": 0.9180694818496704,
      "avg_total": 0.9857676896181973,
      "sample_size": 4
    },
    "natsec_7": {
      "answer_relevancy": 0.8701534280630169,
      "faithfulness": 0.9629629629629629,
      "context_recall": 1.0,
      "context_precision": 0.8569444443995624,
      "answer_correctness": 0.955934839523871,
      "EM": 0.0,
      "F1": 0.6441309780997161,
      "avg_retrieve_context": 0.06769820776852695,
      "avg_llm_response": 1.732023557027181,
      "avg_total": 1.7997217647957078,
      "sample_size": 3
    },
    "natsec_8": {
      "answer_relevancy": 0.9543258034073421,
      "faithfulness": 1.0,
      "context_recall": 1.0,
      "context_precision": 0.6458333333135416,
      "answer_correctness": 0.9049240354907382,
      "EM": 0.0,
      "F1": 0.39925373134328357,
      "avg_retrieve_context": 0.06769820776852695,
      "avg_llm_response": 1.0761929750442505,
      "avg_total": 1.1438911828127774,
      "sample_size": 2
    },
    "negative_rejection": {
      "answer_relevancy": 0.09524508963482156,
      "faithfulness": 0.65,
      "context_recall": 0.8,
      "context_precision": 0.0,
      "answer_correctness": 0.8800968546657458,
      "EM": 0.8,
      "F1": 0.896888888888889,
      "avg_retrieve_context": 0.06769820776852695,
      "avg_llm_response": 0.8396812677383423,
      "avg_total": 0.9073794755068694,
      "sample_size": 10
    }
  },
  "RFP_type": {
    "infra": {
      "answer_relevancy": 0.7570032187352151,
      "faithfulness": 0.8064814814814815,
      "context_recall": 0.9388888888888889,
      "context_precision": 0.7782062756809895,
      "answer_correctness": 0.6673520472556412,
      "EM": 0.016666666666666666,
      "F1": 0.4456061051502262,
      "avg_retrieve_context": 0.06769820776852695,
      "avg_llm_response": 1.465652628739675,
      "avg_total": 1.533350836508202,
      "sample_size": 60
    },
    "natsec": {
      "answer_relevancy": 0.8496607177502437,
      "faithfulness": 0.8826388888888888,
      "context_recall": 0.925,
      "context_precision": 0.7433452380603429,
      "answer_correctness": 0.7532073993389995,
      "EM": 0.025,
      "F1": 0.49405494491420293,
      "avg_retrieve_context": 0.06769820776852695,
      "avg_llm_response": 1.1186675012111664,
      "avg_total": 1.1863657089796935,
      "sample_size": 40
    },
    "negative_rejection": {
      "answer_relevancy": 0.09524508963482156,
      "faithfulness": 0.65,
      "context_recall": 0.8,
      "context_precision": 0.0,
      "answer_correctness": 0.8800968546657458,
      "EM": 0.8,
      "F1": 0.896888888888889,
      "avg_retrieve_context": 0.06769820776852695,
      "avg_llm_response": 0.8396812677383423,
      "avg_total": 0.9073794755068694,
      "sample_size": 10
    }
  },
  "chunks_length": {
    "0": {
      "answer_relevancy": 0.09524508963482156,
      "faithfulness": 0.65,
      "context_recall": 0.8,
      "context_precision": 0.0,
      "answer_correctness": 0.8800968546657458,
      "EM": 0.8,
      "F1": 0.896888888888889,
      "avg_retrieve_context": 0.06769820776852695,
      "avg_llm_response": 0.8396812677383423,
      "avg_total": 0.9073794755068694,
      "sample_size": 10
    },
    "1": {
      "answer_relevancy": 0.8420182014606317,
      "faithfulness": 0.8537851037851037,
      "context_recall": 0.9340659340659341,
      "context_precision": 0.7536638757679235,
      "answer_correctness": 0.7242515002740858,
      "EM": 0.02197802197802198,
      "F1": 0.4914298566316561,
      "avg_retrieve_context": 0.06769820776852696,
      "avg_llm_response": 1.302523033959525,
      "avg_total": 1.370221241728052,
      "sample_size": 91
    },
    "2": {
      "answer_relevancy": 0.30921838902279714,
      "faithfulness": 0.6666666666666666,
      "context_recall": 0.9259259259259258,
      "context_precision": 0.8714192631546724,
      "answer_correctness": 0.4736146982174038,
      "EM": 0.0,
      "F1": 0.19760523912233086,
      "avg_retrieve_context": 0.06769820776852695,
      "avg_llm_response": 1.572917964723375,
      "avg_total": 1.6406161724919022,
      "sample_size": 9
    }
  },
  "manually_edited": {
    "false": {
      "answer_relevancy": 0.808151026205641,
      "faithfulness": 0.8484567901234569,
      "context_recall": 0.9296296296296297,
      "context_precision": 0.7667138447598778,
      "answer_correctness": 0.7304791885180157,
      "EM": 0.022222222222222223,
      "F1": 0.4827699925039387,
      "avg_retrieve_context": 0.06769820776852696,
      "avg_llm_response": 1.3327450858222114,
      "avg_total": 1.4004432935907383,
      "sample_size": 90
    },
    "true": {
      "answer_relevancy": 0.3812740185981588,
      "faithfulness": 0.6916666666666667,
      "context_recall": 0.8833333333333332,
      "context_precision": 0.37109700174420396,
      "answer_correctness": 0.6613630194467246,
      "EM": 0.4,
      "F1": 0.6009076834558046,
      "avg_retrieve_context": 0.06769820776852693,
      "avg_llm_response": 1.0567806363105774,
      "avg_total": 1.124478844079104,
      "sample_size": 20
    }
  },
  "total_negative_questions": 10,
  "total_negative_rejections": 9,
  "negative_rejection_percentage": 90.0
}