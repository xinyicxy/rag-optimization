{
  "full_dataset": {
    "full_dataset": {
      "answer_relevancy": 0.6231354808339931,
      "faithfulness": 0.7756060606060605,
      "context_recall": 0.7897727272727273,
      "context_precision": 0.7272727272186363,
      "answer_correctness": 0.6312890754891545,
      "EM": 0.07272727272727272,
      "F1": 0.4386929427867916,
      "avg_retrieve_context": 0.061655152927745455,
      "avg_llm_response": 1.0982576782053166,
      "avg_total": 1.1599128311330618,
      "sample_size": 110
    }
  },
  "RFP_id": {
    "infra_1": {
      "answer_relevancy": 0.6686239774970866,
      "faithfulness": 0.8492063492063493,
      "context_recall": 0.738095238095238,
      "context_precision": 0.9047619046928569,
      "answer_correctness": 0.5897450938906775,
      "EM": 0.0,
      "F1": 0.289431550088874,
      "avg_retrieve_context": 0.061655152927745455,
      "avg_llm_response": 1.1186340763455345,
      "avg_total": 1.18028922927328,
      "sample_size": 21
    },
    "infra_2": {
      "answer_relevancy": 0.785139278629621,
      "faithfulness": 0.7948717948717948,
      "context_recall": 0.7692307692307693,
      "context_precision": 0.9230769230115382,
      "answer_correctness": 0.6455800851098353,
      "EM": 0.0,
      "F1": 0.5005918088859673,
      "avg_retrieve_context": 0.061655152927745475,
      "avg_llm_response": 1.752158054938683,
      "avg_total": 1.8138132078664286,
      "sample_size": 13
    },
    "infra_3": {
      "answer_relevancy": 0.7248843765448472,
      "faithfulness": 0.75,
      "context_recall": 1.0,
      "context_precision": 0.8333333332708334,
      "answer_correctness": 0.6529068125136336,
      "EM": 0.0,
      "F1": 0.39828116886306536,
      "avg_retrieve_context": 0.061655152927745475,
      "avg_llm_response": 0.6441779931386312,
      "avg_total": 0.7058331460663765,
      "sample_size": 12
    },
    "infra_4": {
      "answer_relevancy": 0.3940965231869958,
      "faithfulness": 0.630952380952381,
      "context_recall": 0.6696428571428571,
      "context_precision": 0.678571428525,
      "answer_correctness": 0.4184492677671142,
      "EM": 0.07142857142857142,
      "F1": 0.3474067764265998,
      "avg_retrieve_context": 0.061655152927745475,
      "avg_llm_response": 0.97138626234872,
      "avg_total": 1.0330414152764653,
      "sample_size": 14
    },
    "natsec_1": {
      "answer_relevancy": 0.645417931299116,
      "faithfulness": 0.7916666666666666,
      "context_recall": 0.8333333333333334,
      "context_precision": 0.8333333332666667,
      "answer_correctness": 0.6456843780301691,
      "EM": 0.0,
      "F1": 0.44160209987137305,
      "avg_retrieve_context": 0.061655152927745475,
      "avg_llm_response": 0.6741751432418823,
      "avg_total": 0.7358302961696278,
      "sample_size": 6
    },
    "natsec_2": {
      "answer_relevancy": 0.7330292215371785,
      "faithfulness": 0.6799999999999999,
      "context_recall": 0.8,
      "context_precision": 0.79999999993,
      "answer_correctness": 0.7547256278455721,
      "EM": 0.0,
      "F1": 0.4514628996925961,
      "avg_retrieve_context": 0.06165515292774547,
      "avg_llm_response": 1.2925774574279785,
      "avg_total": 1.354232610355724,
      "sample_size": 5
    },
    "natsec_3": {
      "answer_relevancy": 0.7183391302701738,
      "faithfulness": 0.875,
      "context_recall": 0.875,
      "context_precision": 0.93749999993125,
      "answer_correctness": 0.7095809015945039,
      "EM": 0.0,
      "F1": 0.4171482846482847,
      "avg_retrieve_context": 0.061655152927745475,
      "avg_llm_response": 0.6630016565322876,
      "avg_total": 0.724656809460033,
      "sample_size": 8
    },
    "natsec_4": {
      "answer_relevancy": 0.7993416343795082,
      "faithfulness": 0.638888888888889,
      "context_recall": 0.5,
      "context_precision": 0.49999999995833333,
      "answer_correctness": 0.534625620903841,
      "EM": 0.0,
      "F1": 0.43784737437407073,
      "avg_retrieve_context": 0.061655152927745475,
      "avg_llm_response": 0.8068744738896688,
      "avg_total": 0.8685296268174142,
      "sample_size": 6
    },
    "natsec_5": {
      "answer_relevancy": 0.7979740938787164,
      "faithfulness": 0.8055555555555555,
      "context_recall": 0.6666666666666666,
      "context_precision": 0.5833333333,
      "answer_correctness": 0.577707635459359,
      "EM": 0.0,
      "F1": 0.40849351804713496,
      "avg_retrieve_context": 0.061655152927745475,
      "avg_llm_response": 1.6834766467412312,
      "avg_total": 1.7451317996689768,
      "sample_size": 6
    },
    "natsec_6": {
      "answer_relevancy": 0.7029509175354134,
      "faithfulness": 0.875,
      "context_recall": 0.75,
      "context_precision": 0.62499999995,
      "answer_correctness": 0.6231647625069809,
      "EM": 0.0,
      "F1": 0.397979797979798,
      "avg_retrieve_context": 0.061655152927745475,
      "avg_llm_response": 0.7583756446838379,
      "avg_total": 0.8200307976115833,
      "sample_size": 4
    },
    "natsec_7": {
      "answer_relevancy": 0.8776106611274025,
      "faithfulness": 1.0,
      "context_recall": 1.0,
      "context_precision": 0.9999999999166667,
      "answer_correctness": 0.9726345103253675,
      "EM": 0.0,
      "F1": 0.617256948441477,
      "avg_retrieve_context": 0.061655152927745475,
      "avg_llm_response": 1.9140506585439045,
      "avg_total": 1.9757058114716501,
      "sample_size": 3
    },
    "natsec_8": {
      "answer_relevancy": 0.47416432816886717,
      "faithfulness": 0.75,
      "context_recall": 0.5,
      "context_precision": 0.499999999975,
      "answer_correctness": 0.5820594989354743,
      "EM": 0.0,
      "F1": 0.24188609920391918,
      "avg_retrieve_context": 0.061655152927745475,
      "avg_llm_response": 1.0102249383926392,
      "avg_total": 1.0718800913203845,
      "sample_size": 2
    },
    "negative_rejection": {
      "answer_relevancy": 0.08197994819063294,
      "faithfulness": 0.75,
      "context_recall": 1.0,
      "context_precision": 0.0,
      "answer_correctness": 0.8398375989552642,
      "EM": 0.7,
      "F1": 0.8777777777777779,
      "avg_retrieve_context": 0.06165515292774547,
      "avg_llm_response": 1.1659268140792847,
      "avg_total": 1.2275819670070303,
      "sample_size": 10
    }
  },
  "RFP_type": {
    "infra": {
      "answer_relevancy": 0.641064633213,
      "faithfulness": 0.7666666666666667,
      "context_recall": 0.78125,
      "context_precision": 0.841666666605,
      "answer_correctness": 0.5745059929505882,
      "EM": 0.016666666666666666,
      "F1": 0.3704804160618852,
      "avg_retrieve_context": 0.06165515292774548,
      "avg_llm_response": 1.1266485651334126,
      "avg_total": 1.1883037180611575,
      "sample_size": 60
    },
    "natsec": {
      "answer_relevancy": 0.7315306354263231,
      "faithfulness": 0.7954166666666665,
      "context_recall": 0.75,
      "context_precision": 0.7374999999437499,
      "answer_correctness": 0.6643265684304771,
      "EM": 0.0,
      "F1": 0.4312405241264048,
      "avg_retrieve_context": 0.06165515292774547,
      "avg_llm_response": 1.0387540638446808,
      "avg_total": 1.1004092167724262,
      "sample_size": 40
    },
    "negative_rejection": {
      "answer_relevancy": 0.08197994819063294,
      "faithfulness": 0.75,
      "context_recall": 1.0,
      "context_precision": 0.0,
      "answer_correctness": 0.8398375989552642,
      "EM": 0.7,
      "F1": 0.8777777777777779,
      "avg_retrieve_context": 0.06165515292774547,
      "avg_llm_response": 1.1659268140792847,
      "avg_total": 1.2275819670070303,
      "sample_size": 10
    }
  },
  "chunks_length": {
    "0": {
      "answer_relevancy": 0.08197994819063294,
      "faithfulness": 0.75,
      "context_recall": 1.0,
      "context_precision": 0.0,
      "answer_correctness": 0.8398375989552642,
      "EM": 0.7,
      "F1": 0.8777777777777779,
      "avg_retrieve_context": 0.06165515292774547,
      "avg_llm_response": 1.1659268140792847,
      "avg_total": 1.2275819670070303,
      "sample_size": 10
    },
    "1": {
      "answer_relevancy": 0.7142597626768574,
      "faithfulness": 0.7891941391941392,
      "context_recall": 0.7934981684981686,
      "context_precision": 0.7802197801598902,
      "answer_correctness": 0.6334013131481265,
      "EM": 0.01098901098901099,
      "F1": 0.42000185332720796,
      "avg_retrieve_context": 0.06165515292774546,
      "avg_llm_response": 1.1046660496638372,
      "avg_total": 1.1663212025915821,
      "sample_size": 91
    },
    "2": {
      "answer_relevancy": 0.303051667359876,
      "faithfulness": 0.6666666666666666,
      "context_recall": 0.5185185185185185,
      "context_precision": 0.9999999999444444,
      "answer_correctness": 0.3782114241972057,
      "EM": 0.0,
      "F1": 0.13980858622148662,
      "avg_retrieve_context": 0.06165515292774547,
      "avg_llm_response": 0.9582739935980903,
      "avg_total": 1.0199291465258358,
      "sample_size": 9
    }
  },
  "manually_edited": {
    "false": {
      "answer_relevancy": 0.6998954831341879,
      "faithfulness": 0.7961111111111112,
      "context_recall": 0.7726851851851853,
      "context_precision": 0.7944444443850002,
      "answer_correctness": 0.6306571359511113,
      "EM": 0.011111111111111112,
      "F1": 0.4099851090033215,
      "avg_retrieve_context": 0.06165515292774547,
      "avg_llm_response": 1.1312670124901665,
      "avg_total": 1.1929221654179116,
      "sample_size": 90
    },
    "true": {
      "answer_relevancy": 0.27771547048311707,
      "faithfulness": 0.6833333333333333,
      "context_recall": 0.8666666666666666,
      "context_precision": 0.42499999997,
      "answer_correctness": 0.6341328034103496,
      "EM": 0.35,
      "F1": 0.5678781948124075,
      "avg_retrieve_context": 0.06165515292774546,
      "avg_llm_response": 0.9497156739234924,
      "avg_total": 1.011370826851238,
      "sample_size": 20
    }
  },
  "total_negative_questions": 10,
  "total_negative_rejections": 9,
  "negative_rejection_percentage": 90.0
}