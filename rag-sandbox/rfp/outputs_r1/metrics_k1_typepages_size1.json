{
  "full_dataset": {
    "full_dataset": {
      "answer_relevancy": 0.4886226877344171,
      "faithfulness": 0.725058275058275,
      "context_recall": 0.6439393939393939,
      "context_precision": 0.67272727266,
      "answer_correctness": 0.5460700803409293,
      "EM": 0.05454545454545454,
      "F1": 0.35879858073100807,
      "avg_retrieve_context": 0.054064210978421325,
      "avg_llm_response": 1.0278295733711935,
      "avg_total": 1.081893784349615,
      "sample_size": 110
    }
  },
  "RFP_id": {
    "infra_1": {
      "answer_relevancy": 0.5678175171369408,
      "faithfulness": 0.7185592185592186,
      "context_recall": 0.4920634920634921,
      "context_precision": 0.8095238094428571,
      "answer_correctness": 0.5023333117947365,
      "EM": 0.0,
      "F1": 0.26192389274852035,
      "avg_retrieve_context": 0.054064210978421304,
      "avg_llm_response": 1.8210358052026658,
      "avg_total": 1.8751000161810876,
      "sample_size": 21
    },
    "infra_2": {
      "answer_relevancy": 0.4270303654013156,
      "faithfulness": 0.6666666666666667,
      "context_recall": 0.6538461538461539,
      "context_precision": 0.6153846153230769,
      "answer_correctness": 0.43458501980273456,
      "EM": 0.07692307692307693,
      "F1": 0.2733369596824605,
      "avg_retrieve_context": 0.0540642109784213,
      "avg_llm_response": 1.2671208381652832,
      "avg_total": 1.3211850491437043,
      "sample_size": 13
    },
    "infra_3": {
      "answer_relevancy": 0.7258786730577684,
      "faithfulness": 0.8333333333333334,
      "context_recall": 0.9166666666666666,
      "context_precision": 0.916666666575,
      "answer_correctness": 0.6842591139635492,
      "EM": 0.0,
      "F1": 0.39733102794968916,
      "avg_retrieve_context": 0.0540642109784213,
      "avg_llm_response": 0.5491353273391724,
      "avg_total": 0.6031995383175935,
      "sample_size": 12
    },
    "infra_4": {
      "answer_relevancy": 0.2620541243111401,
      "faithfulness": 0.5952380952380951,
      "context_recall": 0.5,
      "context_precision": 0.6428571427928571,
      "answer_correctness": 0.35839350383482194,
      "EM": 0.0,
      "F1": 0.20624396843967174,
      "avg_retrieve_context": 0.0540642109784213,
      "avg_llm_response": 0.6717171328408378,
      "avg_total": 0.7257813438192589,
      "sample_size": 14
    },
    "natsec_1": {
      "answer_relevancy": 0.4957143611990737,
      "faithfulness": 0.6666666666666666,
      "context_recall": 0.5,
      "context_precision": 0.49999999995,
      "answer_correctness": 0.5852417318822657,
      "EM": 0.0,
      "F1": 0.34325880059626196,
      "avg_retrieve_context": 0.0540642109784213,
      "avg_llm_response": 0.7306861480077108,
      "avg_total": 0.7847503589861319,
      "sample_size": 6
    },
    "natsec_2": {
      "answer_relevancy": 0.7621991890699907,
      "faithfulness": 0.9,
      "context_recall": 0.8,
      "context_precision": 0.79999999992,
      "answer_correctness": 0.5996047561986912,
      "EM": 0.0,
      "F1": 0.44962993995252054,
      "avg_retrieve_context": 0.054064210978421304,
      "avg_llm_response": 1.2542482376098634,
      "avg_total": 1.3083124485882844,
      "sample_size": 5
    },
    "natsec_3": {
      "answer_relevancy": 0.6046369488000003,
      "faithfulness": 0.8125,
      "context_recall": 0.75,
      "context_precision": 0.8749999999125,
      "answer_correctness": 0.585432047720942,
      "EM": 0.0,
      "F1": 0.3521362619517917,
      "avg_retrieve_context": 0.0540642109784213,
      "avg_llm_response": 0.7146298289299011,
      "avg_total": 0.7686940399083224,
      "sample_size": 8
    },
    "natsec_4": {
      "answer_relevancy": 0.49276399152517847,
      "faithfulness": 0.6666666666666666,
      "context_recall": 0.5833333333333334,
      "context_precision": 0.6666666666,
      "answer_correctness": 0.49981626293778386,
      "EM": 0.0,
      "F1": 0.36393613554903875,
      "avg_retrieve_context": 0.0540642109784213,
      "avg_llm_response": 0.4415598313013713,
      "avg_total": 0.49562404227979256,
      "sample_size": 6
    },
    "natsec_5": {
      "answer_relevancy": 0.6343379155456655,
      "faithfulness": 0.7222222222222222,
      "context_recall": 0.6666666666666666,
      "context_precision": 0.6666666666,
      "answer_correctness": 0.5766701455360135,
      "EM": 0.0,
      "F1": 0.3437997519776573,
      "avg_retrieve_context": 0.0540642109784213,
      "avg_llm_response": 1.471748153368632,
      "avg_total": 1.5258123643470531,
      "sample_size": 6
    },
    "natsec_6": {
      "answer_relevancy": 0.23713939599203906,
      "faithfulness": 0.7083333333333333,
      "context_recall": 0.375,
      "context_precision": 0.49999999995,
      "answer_correctness": 0.3479054283489045,
      "EM": 0.0,
      "F1": 0.2048523748395379,
      "avg_retrieve_context": 0.0540642109784213,
      "avg_llm_response": 0.42049652338027954,
      "avg_total": 0.47456073435870083,
      "sample_size": 4
    },
    "natsec_7": {
      "answer_relevancy": 0.8962481165904631,
      "faithfulness": 1.0,
      "context_recall": 1.0,
      "context_precision": 0.9999999999,
      "answer_correctness": 0.9562958563096836,
      "EM": 0.0,
      "F1": 0.5693843193843193,
      "avg_retrieve_context": 0.0540642109784213,
      "avg_llm_response": 1.7065767447153728,
      "avg_total": 1.760640955693794,
      "sample_size": 3
    },
    "natsec_8": {
      "answer_relevancy": 0.49190378416095876,
      "faithfulness": 0.75,
      "context_recall": 0.5,
      "context_precision": 0.9999999999,
      "answer_correctness": 0.32842764847329864,
      "EM": 0.0,
      "F1": 0.15217391304347827,
      "avg_retrieve_context": 0.0540642109784213,
      "avg_llm_response": 0.49642908573150635,
      "avg_total": 0.5504932967099276,
      "sample_size": 2
    },
    "negative_rejection": {
      "answer_relevancy": 0.0887532576212915,
      "faithfulness": 0.7,
      "context_recall": 0.8,
      "context_precision": 0.0,
      "answer_correctness": 0.8071272026958474,
      "EM": 0.5,
      "F1": 0.8555555555555555,
      "avg_retrieve_context": 0.054064210978421304,
      "avg_llm_response": 0.670644736289978,
      "avg_total": 0.7247089472683992,
      "sample_size": 10
    }
  },
  "RFP_type": {
    "infra": {
      "answer_relevancy": 0.4975810737857007,
      "faithfulness": 0.7014957264957266,
      "context_recall": 0.6138888888888888,
      "context_precision": 0.7499999999249999,
      "answer_correctness": 0.4904537204395852,
      "EM": 0.016666666666666666,
      "F1": 0.2784861686190432,
      "avg_retrieve_context": 0.0540642109784213,
      "avg_llm_response": 1.1784664432207743,
      "avg_total": 1.2325306541991958,
      "sample_size": 60
    },
    "natsec": {
      "answer_relevancy": 0.5751524661857731,
      "faithfulness": 0.7666666666666666,
      "context_recall": 0.65,
      "context_precision": 0.7249999999275,
      "answer_correctness": 0.5642303396042159,
      "EM": 0.0,
      "F1": 0.3550779551928187,
      "avg_retrieve_context": 0.054064210978421304,
      "avg_llm_response": 0.8911704778671264,
      "avg_total": 0.9452346888455476,
      "sample_size": 40
    },
    "negative_rejection": {
      "answer_relevancy": 0.0887532576212915,
      "faithfulness": 0.7,
      "context_recall": 0.8,
      "context_precision": 0.0,
      "answer_correctness": 0.8071272026958474,
      "EM": 0.5,
      "F1": 0.8555555555555555,
      "avg_retrieve_context": 0.054064210978421304,
      "avg_llm_response": 0.670644736289978,
      "avg_total": 0.7247089472683992,
      "sample_size": 10
    }
  },
  "chunks_length": {
    "0": {
      "answer_relevancy": 0.0887532576212915,
      "faithfulness": 0.7,
      "context_recall": 0.8,
      "context_precision": 0.0,
      "answer_correctness": 0.8071272026958474,
      "EM": 0.5,
      "F1": 0.8555555555555555,
      "avg_retrieve_context": 0.054064210978421304,
      "avg_llm_response": 0.670644736289978,
      "avg_total": 0.7247089472683992,
      "sample_size": 10
    },
    "1": {
      "answer_relevancy": 0.5410678621440955,
      "faithfulness": 0.7417582417582419,
      "context_recall": 0.6538461538461539,
      "context_precision": 0.7252747252021977,
      "answer_correctness": 0.5340724874297821,
      "EM": 0.01098901098901099,
      "F1": 0.32664811411032774,
      "avg_retrieve_context": 0.05406421097842132,
      "avg_llm_response": 1.045547637310657,
      "avg_total": 1.0996118482890782,
      "sample_size": 91
    },
    "2": {
      "answer_relevancy": 0.4026430688289197,
      "faithfulness": 0.584045584045584,
      "context_recall": 0.37037037037037035,
      "context_precision": 0.8888888887999999,
      "answer_correctness": 0.37731560604817466,
      "EM": 0.0,
      "F1": 0.1319233267572798,
      "avg_retrieve_context": 0.0540642109784213,
      "avg_llm_response": 1.2455523014068604,
      "avg_total": 1.2996165123852816,
      "sample_size": 9
    }
  },
  "manually_edited": {
    "false": {
      "answer_relevancy": 0.5228080477677289,
      "faithfulness": 0.7240740740740741,
      "context_recall": 0.6055555555555555,
      "context_precision": 0.7111111110399999,
      "answer_correctness": 0.5178587861458139,
      "EM": 0.011111111111111112,
      "F1": 0.3141149957365276,
      "avg_retrieve_context": 0.05406421097842131,
      "avg_llm_response": 1.1041741953955757,
      "avg_total": 1.1582384063739968,
      "sample_size": 90
    },
    "true": {
      "answer_relevancy": 0.3347885675845138,
      "faithfulness": 0.7294871794871794,
      "context_recall": 0.8166666666666667,
      "context_precision": 0.49999999995,
      "answer_correctness": 0.6730209042189483,
      "EM": 0.25,
      "F1": 0.5598747132061705,
      "avg_retrieve_context": 0.054064210978421304,
      "avg_llm_response": 0.6842787742614747,
      "avg_total": 0.7383429852398958,
      "sample_size": 20
    }
  },
  "total_negative_questions": 10,
  "total_negative_rejections": 9,
  "negative_rejection_percentage": 90.0
}