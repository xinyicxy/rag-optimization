{
  "full_dataset": {
    "full_dataset": {
      "answer_relevancy": 0.6670389983964341,
      "faithfulness": 0.7986068476977568,
      "context_recall": 0.8287878787878789,
      "context_precision": 0.8181818181259091,
      "answer_correctness": 0.6503185616365961,
      "EM": 0.045454545454545456,
      "F1": 0.45805402373193155,
      "avg_retrieve_context": 0.053871527585116284,
      "avg_llm_response": 1.2717949108643964,
      "avg_total": 1.3256664384495134,
      "sample_size": 110
    }
  },
  "RFP_id": {
    "infra_1": {
      "answer_relevancy": 0.7140922569791505,
      "faithfulness": 0.800206143063286,
      "context_recall": 0.8095238095238095,
      "context_precision": 0.9047619047047617,
      "answer_correctness": 0.6183222956132013,
      "EM": 0.0,
      "F1": 0.27704548964659387,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 1.8892405827840169,
      "avg_total": 1.943112110369133,
      "sample_size": 21
    },
    "infra_2": {
      "answer_relevancy": 0.7861268982722424,
      "faithfulness": 0.8083916083916084,
      "context_recall": 0.9230769230769231,
      "context_precision": 0.9999999999384616,
      "answer_correctness": 0.6946373798575186,
      "EM": 0.07692307692307693,
      "F1": 0.5759017986594184,
      "avg_retrieve_context": 0.05387152758511629,
      "avg_llm_response": 1.8952791324028602,
      "avg_total": 1.9491506599879764,
      "sample_size": 13
    },
    "infra_3": {
      "answer_relevancy": 0.6242507531811897,
      "faithfulness": 0.8333333333333334,
      "context_recall": 0.8333333333333334,
      "context_precision": 0.87499999995,
      "answer_correctness": 0.5432313311563173,
      "EM": 0.0,
      "F1": 0.3620039636373575,
      "avg_retrieve_context": 0.05387152758511629,
      "avg_llm_response": 0.7952101230621338,
      "avg_total": 0.8490816506472499,
      "sample_size": 12
    },
    "infra_4": {
      "answer_relevancy": 0.5958502194383455,
      "faithfulness": 0.7142857142857143,
      "context_recall": 0.75,
      "context_precision": 0.8928571427892856,
      "answer_correctness": 0.5340728622525853,
      "EM": 0.0,
      "F1": 0.43434821593319695,
      "avg_retrieve_context": 0.053871527585116284,
      "avg_llm_response": 1.058387177331107,
      "avg_total": 1.1122587049162234,
      "sample_size": 14
    },
    "natsec_1": {
      "answer_relevancy": 0.9038449144262364,
      "faithfulness": 1.0,
      "context_recall": 1.0,
      "context_precision": 0.833333333275,
      "answer_correctness": 0.8735470567125133,
      "EM": 0.0,
      "F1": 0.4188583220841286,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 0.8414576848347982,
      "avg_total": 0.8953292124199144,
      "sample_size": 6
    },
    "natsec_2": {
      "answer_relevancy": 0.7518646388222565,
      "faithfulness": 0.64,
      "context_recall": 0.6,
      "context_precision": 0.99999999991,
      "answer_correctness": 0.528174164454288,
      "EM": 0.0,
      "F1": 0.4214599005209362,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 1.2561236381530763,
      "avg_total": 1.3099951657381925,
      "sample_size": 5
    },
    "natsec_3": {
      "answer_relevancy": 0.5917543212264672,
      "faithfulness": 0.8125,
      "context_recall": 0.875,
      "context_precision": 0.99999999993125,
      "answer_correctness": 0.6084370898275502,
      "EM": 0.0,
      "F1": 0.34843479964892443,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 0.8005356192588806,
      "avg_total": 0.8544071468439969,
      "sample_size": 8
    },
    "natsec_4": {
      "answer_relevancy": 0.8023416930883478,
      "faithfulness": 0.8333333333333334,
      "context_recall": 0.8333333333333334,
      "context_precision": 0.7499999999333333,
      "answer_correctness": 0.6628718895580933,
      "EM": 0.0,
      "F1": 0.5444916911045944,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 0.6670765479405721,
      "avg_total": 0.7209480755256883,
      "sample_size": 6
    },
    "natsec_5": {
      "answer_relevancy": 0.7679451832647382,
      "faithfulness": 0.9166666666666666,
      "context_recall": 0.611111111111111,
      "context_precision": 0.8333333332833334,
      "answer_correctness": 0.504966869258422,
      "EM": 0.0,
      "F1": 0.38813178780284036,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 1.796738823254903,
      "avg_total": 1.8506103508400196,
      "sample_size": 6
    },
    "natsec_6": {
      "answer_relevancy": 0.9200284879229091,
      "faithfulness": 0.9166666666666666,
      "context_recall": 1.0,
      "context_precision": 0.7499999999375,
      "answer_correctness": 0.7157186342626044,
      "EM": 0.0,
      "F1": 0.6405612244897959,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 0.8765829205513,
      "avg_total": 0.9304544481364163,
      "sample_size": 4
    },
    "natsec_7": {
      "answer_relevancy": 0.8770873982168635,
      "faithfulness": 1.0,
      "context_recall": 1.0,
      "context_precision": 0.9999999999333333,
      "answer_correctness": 0.9361748915340892,
      "EM": 0.0,
      "F1": 0.6448540706605222,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 1.6937789122263591,
      "avg_total": 1.7476504398114754,
      "sample_size": 3
    },
    "natsec_8": {
      "answer_relevancy": 0.8943667040206182,
      "faithfulness": 0.8333333333333333,
      "context_recall": 1.0,
      "context_precision": 0.74999999995,
      "answer_correctness": 0.6890099454080268,
      "EM": 0.0,
      "F1": 0.3557105492589363,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 1.3197308778762817,
      "avg_total": 1.3736024054613982,
      "sample_size": 2
    },
    "negative_rejection": {
      "answer_relevancy": 0.08875325762129149,
      "faithfulness": 0.6,
      "context_recall": 0.8,
      "context_precision": 0.0,
      "answer_correctness": 0.8718090052053856,
      "EM": 0.4,
      "F1": 0.8444444444444444,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 0.7481145620346069,
      "avg_total": 0.8019860896197233,
      "sample_size": 10
    }
  },
  "RFP_type": {
    "infra": {
      "answer_relevancy": 0.6841416530735404,
      "faithfulness": 0.7885569985569986,
      "context_recall": 0.825,
      "context_precision": 0.9166666666074998,
      "answer_correctness": 0.6001808365239495,
      "EM": 0.016666666666666666,
      "F1": 0.3954933541977326,
      "avg_retrieve_context": 0.05387152758511631,
      "avg_llm_response": 1.4778770486513773,
      "avg_total": 1.5317485762364933,
      "sample_size": 60
    },
    "natsec": {
      "answer_relevancy": 0.7859564515745604,
      "faithfulness": 0.8633333333333333,
      "context_recall": 0.8416666666666666,
      "context_precision": 0.8749999999349999,
      "answer_correctness": 0.6701525384133689,
      "EM": 0.0,
      "F1": 0.455297422855102,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 1.0935917913913726,
      "avg_total": 1.1474633189764891,
      "sample_size": 40
    },
    "negative_rejection": {
      "answer_relevancy": 0.08875325762129149,
      "faithfulness": 0.6,
      "context_recall": 0.8,
      "context_precision": 0.0,
      "answer_correctness": 0.8718090052053856,
      "EM": 0.4,
      "F1": 0.8444444444444444,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 0.7481145620346069,
      "avg_total": 0.8019860896197233,
      "sample_size": 10
    }
  },
  "chunks_length": {
    "0": {
      "answer_relevancy": 0.08875325762129149,
      "faithfulness": 0.6,
      "context_recall": 0.8,
      "context_precision": 0.0,
      "answer_correctness": 0.8718090052053856,
      "EM": 0.4,
      "F1": 0.8444444444444444,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 0.7481145620346069,
      "avg_total": 0.8019860896197233,
      "sample_size": 10
    },
    "1": {
      "answer_relevancy": 0.7374264449485822,
      "faithfulness": 0.8264735264735265,
      "context_recall": 0.8479853479853481,
      "context_precision": 0.8901098900472527,
      "answer_correctness": 0.6378366565726834,
      "EM": 0.01098901098901099,
      "F1": 0.4417766569462276,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 1.2685976316640666,
      "avg_total": 1.3224691592491826,
      "sample_size": 91
    },
    "2": {
      "answer_relevancy": 0.5978834174526503,
      "faithfulness": 0.7375180375180376,
      "context_recall": 0.6666666666666666,
      "context_precision": 0.9999999999499999,
      "answer_correctness": 0.530423997761948,
      "EM": 0.0,
      "F1": 0.1933135982179249,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 1.8859900103674994,
      "avg_total": 1.9398615379526154,
      "sample_size": 9
    }
  },
  "manually_edited": {
    "false": {
      "answer_relevancy": 0.7328887489185535,
      "faithfulness": 0.825016835016835,
      "context_recall": 0.824074074074074,
      "context_precision": 0.8999999999372222,
      "answer_correctness": 0.6328081581805773,
      "EM": 0.011111111111111112,
      "F1": 0.42527979248534836,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 1.35763738155365,
      "avg_total": 1.411508909138766,
      "sample_size": 90
    },
    "true": {
      "answer_relevancy": 0.37071512104689724,
      "faithfulness": 0.6797619047619048,
      "context_recall": 0.85,
      "context_precision": 0.449999999975,
      "answer_correctness": 0.7291153771886811,
      "EM": 0.2,
      "F1": 0.6055380643415562,
      "avg_retrieve_context": 0.0538715275851163,
      "avg_llm_response": 0.8855037927627564,
      "avg_total": 0.9393753203478727,
      "sample_size": 20
    }
  },
  "total_negative_questions": 10,
  "total_negative_rejections": 9,
  "negative_rejection_percentage": 90.0
}