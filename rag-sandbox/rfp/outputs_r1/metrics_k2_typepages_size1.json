{
  "full_dataset": {
    "full_dataset": {
      "answer_relevancy": 0.6620743489691435,
      "faithfulness": 0.8159090909090909,
      "context_recall": 0.8045454545454546,
      "context_precision": 0.7454545454018181,
      "answer_correctness": 0.6360323134699165,
      "EM": 0.045454545454545456,
      "F1": 0.43580291603814875,
      "avg_retrieve_context": 0.08904550942507661,
      "avg_llm_response": 1.104470419883728,
      "avg_total": 1.193515929308804,
      "sample_size": 110
    }
  },
  "RFP_id": {
    "infra_1": {
      "answer_relevancy": 0.6629175169870949,
      "faithfulness": 0.8571428571428571,
      "context_recall": 0.7619047619047619,
      "context_precision": 0.8333333332738094,
      "answer_correctness": 0.540210166243518,
      "EM": 0.0,
      "F1": 0.28312016159829,
      "avg_retrieve_context": 0.08904550942507658,
      "avg_llm_response": 1.1690950280144101,
      "avg_total": 1.258140537439487,
      "sample_size": 21
    },
    "infra_2": {
      "answer_relevancy": 0.7121781477951005,
      "faithfulness": 0.7564102564102565,
      "context_recall": 0.7692307692307693,
      "context_precision": 0.7307692307153845,
      "answer_correctness": 0.6370301261506884,
      "EM": 0.0,
      "F1": 0.45193487019444933,
      "avg_retrieve_context": 0.08904550942507658,
      "avg_llm_response": 1.3698093524345984,
      "avg_total": 1.458854861859675,
      "sample_size": 13
    },
    "infra_3": {
      "answer_relevancy": 0.7155981678791247,
      "faithfulness": 0.8541666666666666,
      "context_recall": 0.8333333333333334,
      "context_precision": 0.9166666666041667,
      "answer_correctness": 0.6326714684699496,
      "EM": 0.0,
      "F1": 0.3716533069396892,
      "avg_retrieve_context": 0.08904550942507657,
      "avg_llm_response": 0.8184121052424113,
      "avg_total": 0.9074576146674879,
      "sample_size": 12
    },
    "infra_4": {
      "answer_relevancy": 0.5990436034215855,
      "faithfulness": 0.8214285714285714,
      "context_recall": 0.75,
      "context_precision": 0.8571428570749998,
      "answer_correctness": 0.5535466577570285,
      "EM": 0.0,
      "F1": 0.4748281814165889,
      "avg_retrieve_context": 0.08904550942507658,
      "avg_llm_response": 1.4708448648452759,
      "avg_total": 1.5598903742703527,
      "sample_size": 14
    },
    "natsec_1": {
      "answer_relevancy": 0.6096203342449705,
      "faithfulness": 0.6388888888888888,
      "context_recall": 0.8333333333333334,
      "context_precision": 0.666666666625,
      "answer_correctness": 0.6744443996512524,
      "EM": 0.0,
      "F1": 0.35629629629629633,
      "avg_retrieve_context": 0.08904550942507657,
      "avg_llm_response": 0.8347610632578532,
      "avg_total": 0.9238065726829299,
      "sample_size": 6
    },
    "natsec_2": {
      "answer_relevancy": 0.7509212564941669,
      "faithfulness": 0.9,
      "context_recall": 0.8,
      "context_precision": 0.79999999994,
      "answer_correctness": 0.6517711218804182,
      "EM": 0.0,
      "F1": 0.4295642763034067,
      "avg_retrieve_context": 0.08904550942507657,
      "avg_llm_response": 1.4117275714874267,
      "avg_total": 1.5007730809125033,
      "sample_size": 5
    },
    "natsec_3": {
      "answer_relevancy": 0.9565867830324661,
      "faithfulness": 0.96875,
      "context_recall": 1.0,
      "context_precision": 0.93749999994375,
      "answer_correctness": 0.8094022013805553,
      "EM": 0.0,
      "F1": 0.5388090858214039,
      "avg_retrieve_context": 0.08904550942507657,
      "avg_llm_response": 0.8523687720298767,
      "avg_total": 0.9414142814549533,
      "sample_size": 8
    },
    "natsec_4": {
      "answer_relevancy": 0.6159972981069839,
      "faithfulness": 0.7916666666666666,
      "context_recall": 0.6666666666666666,
      "context_precision": 0.666666666625,
      "answer_correctness": 0.5978837352226628,
      "EM": 0.0,
      "F1": 0.36022028191519717,
      "avg_retrieve_context": 0.08904550942507657,
      "avg_llm_response": 0.569482684135437,
      "avg_total": 0.6585281935605136,
      "sample_size": 6
    },
    "natsec_5": {
      "answer_relevancy": 0.9541872393775531,
      "faithfulness": 1.0,
      "context_recall": 0.8333333333333334,
      "context_precision": 0.7499999999416667,
      "answer_correctness": 0.7512232990573643,
      "EM": 0.0,
      "F1": 0.5310681801671504,
      "avg_retrieve_context": 0.08904550942507657,
      "avg_llm_response": 1.5409908692042034,
      "avg_total": 1.6300363786292797,
      "sample_size": 6
    },
    "natsec_6": {
      "answer_relevancy": 0.4383277430880603,
      "faithfulness": 0.7083333333333333,
      "context_recall": 0.5,
      "context_precision": 0.7499999999625,
      "answer_correctness": 0.2819004900876312,
      "EM": 0.0,
      "F1": 0.18322055137844612,
      "avg_retrieve_context": 0.08904550942507657,
      "avg_llm_response": 0.5578102469444275,
      "avg_total": 0.6468557563695041,
      "sample_size": 4
    },
    "natsec_7": {
      "answer_relevancy": 0.8918016408670653,
      "faithfulness": 1.0,
      "context_recall": 1.0,
      "context_precision": 0.9999999999166667,
      "answer_correctness": 0.9578825584147306,
      "EM": 0.0,
      "F1": 0.6701298701298701,
      "avg_retrieve_context": 0.08904550942507657,
      "avg_llm_response": 1.7035311063130696,
      "avg_total": 1.7925766157381462,
      "sample_size": 3
    },
    "natsec_8": {
      "answer_relevancy": 0.9528564165087918,
      "faithfulness": 0.75,
      "context_recall": 1.0,
      "context_precision": 0.999999999925,
      "answer_correctness": 0.3832919528684398,
      "EM": 0.0,
      "F1": 0.29119732999582815,
      "avg_retrieve_context": 0.08904550942507657,
      "avg_llm_response": 0.8888164758682251,
      "avg_total": 0.9778619852933017,
      "sample_size": 2
    },
    "negative_rejection": {
      "answer_relevancy": 0.18542476410101544,
      "faithfulness": 0.6,
      "context_recall": 0.9,
      "context_precision": 0.0,
      "answer_correctness": 0.8352823155014415,
      "EM": 0.5,
      "F1": 0.774074074074074,
      "avg_retrieve_context": 0.08904550942507658,
      "avg_llm_response": 0.8051992654800415,
      "avg_total": 0.894244774905118,
      "sample_size": 10
    }
  },
  "RFP_type": {
    "infra": {
      "answer_relevancy": 0.6692228706752831,
      "faithfulness": 0.8263888888888888,
      "context_recall": 0.775,
      "context_precision": 0.8333333332724998,
      "answer_correctness": 0.5827919326885103,
      "EM": 0.0,
      "F1": 0.3821351821533408,
      "avg_retrieve_context": 0.0890455094250766,
      "avg_llm_response": 1.21285484234492,
      "avg_total": 1.3019003517699967,
      "sample_size": 60
    },
    "natsec": {
      "answer_relevancy": 0.7705139626269657,
      "faithfulness": 0.8541666666666666,
      "context_recall": 0.825,
      "context_precision": 0.7999999999462502,
      "answer_correctness": 0.6660803841341452,
      "EM": 0.0,
      "F1": 0.43173672735637947,
      "avg_retrieve_context": 0.08904550942507657,
      "avg_llm_response": 1.016711574792862,
      "avg_total": 1.1057570842179385,
      "sample_size": 40
    },
    "negative_rejection": {
      "answer_relevancy": 0.18542476410101544,
      "faithfulness": 0.6,
      "context_recall": 0.9,
      "context_precision": 0.0,
      "answer_correctness": 0.8352823155014415,
      "EM": 0.5,
      "F1": 0.774074074074074,
      "avg_retrieve_context": 0.08904550942507658,
      "avg_llm_response": 0.8051992654800415,
      "avg_total": 0.894244774905118,
      "sample_size": 10
    }
  },
  "chunks_length": {
    "0": {
      "answer_relevancy": 0.18542476410101544,
      "faithfulness": 0.6,
      "context_recall": 0.9,
      "context_precision": 0.0,
      "answer_correctness": 0.8352823155014415,
      "EM": 0.5,
      "F1": 0.774074074074074,
      "avg_retrieve_context": 0.08904550942507658,
      "avg_llm_response": 0.8051992654800415,
      "avg_total": 0.894244774905118,
      "sample_size": 10
    },
    "1": {
      "answer_relevancy": 0.7402000470900346,
      "faithfulness": 0.8489010989010989,
      "context_recall": 0.8150183150183149,
      "context_precision": 0.8186813186225276,
      "answer_correctness": 0.6468878497757279,
      "EM": 0.0,
      "F1": 0.42876864343100063,
      "avg_retrieve_context": 0.0890455094250766,
      "avg_llm_response": 1.142625358078506,
      "avg_total": 1.2316708675035828,
      "sample_size": 91
    },
    "2": {
      "answer_relevancy": 0.40174738448916125,
      "faithfulness": 0.7222222222222222,
      "context_recall": 0.5925925925925926,
      "context_precision": 0.8333333332833334,
      "answer_correctness": 0.3048818885650218,
      "EM": 0.0,
      "F1": 0.13107038569272927,
      "avg_retrieve_context": 0.08904550942507657,
      "avg_llm_response": 1.0512051052517362,
      "avg_total": 1.140250614676813,
      "sample_size": 9
    }
  },
  "manually_edited": {
    "false": {
      "answer_relevancy": 0.7254302649272228,
      "faithfulness": 0.8388888888888889,
      "context_recall": 0.7833333333333333,
      "context_precision": 0.7999999999438889,
      "answer_correctness": 0.625985506543919,
      "EM": 0.0,
      "F1": 0.41517003979537487,
      "avg_retrieve_context": 0.0890455094250766,
      "avg_llm_response": 1.1859725051456027,
      "avg_total": 1.2750180145706795,
      "sample_size": 90
    },
    "true": {
      "answer_relevancy": 0.37697272715778507,
      "faithfulness": 0.7125,
      "context_recall": 0.9,
      "context_precision": 0.49999999996250005,
      "answer_correctness": 0.6812429446369065,
      "EM": 0.25,
      "F1": 0.5286508591306313,
      "avg_retrieve_context": 0.08904550942507658,
      "avg_llm_response": 0.7377110362052918,
      "avg_total": 0.8267565456303683,
      "sample_size": 20
    }
  },
  "total_negative_questions": 10,
  "total_negative_rejections": 8,
  "negative_rejection_percentage": 80.0
}