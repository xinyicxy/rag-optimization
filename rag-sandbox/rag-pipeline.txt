
Thoughts: 

Pipeline Steps
- Load local text files
- Generate embeddings (BAAI/bge-small-en)
- Store in ChromaDB
- Retrieve relevant docs
- Use whatevrr llm for generation

Using rn: 
- Storage: local (but can use azure blob storage for free) -> WANT TO UPGRADE TO OPENAI gpt-4(o?)
- Embeddings (huggingface: BAAI/bge-small-en) -> WANT TO UPGRADE TO OPENAI text-embedding-large
- LLM (huggingface: tinyllama)
